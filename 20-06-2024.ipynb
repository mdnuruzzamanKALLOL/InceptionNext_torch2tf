{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5342045,"sourceType":"datasetVersion","datasetId":3101996},{"sourceId":63685,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":53104},{"sourceId":64136,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":53482}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from functools import partial\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.models.helpers import checkpoint_seq\nfrom timm.models.layers import trunc_normal_, DropPath\nfrom timm.models.registry import register_model\n#from timm.models.layers.helpers import to_2tuple\nfrom timm.models.layers import to_2tuple\n\n\nclass InceptionDWConv2d(nn.Module):\n    def __init__(self, in_channels, square_kernel_size=3, band_kernel_size=11, branch_ratio=0.125):\n        super().__init__()\n        \n        gc = int(in_channels * branch_ratio) # channel numbers of a convolution branch\n        self.dwconv_hw = nn.Conv2d(gc, gc, square_kernel_size, padding=square_kernel_size//2, groups=gc)\n        self.dwconv_w = nn.Conv2d(gc, gc, kernel_size=(1, band_kernel_size), padding=(0, band_kernel_size//2), groups=gc)\n        self.dwconv_h = nn.Conv2d(gc, gc, kernel_size=(band_kernel_size, 1), padding=(band_kernel_size//2, 0), groups=gc)\n        self.split_indexes = (in_channels - 3 * gc, gc, gc, gc)\n        \n    def forward(self, x):\n        x_id, x_hw, x_w, x_h = torch.split(x, self.split_indexes, dim=1)\n        return torch.cat(\n            (x_id, self.dwconv_hw(x_hw), self.dwconv_w(x_w), self.dwconv_h(x_h)), \n            dim=1,\n        )\n\n\nclass ConvMlp(nn.Module):\n    def __init__(\n            self, in_features, hidden_features=None, out_features=None, act_layer=nn.ReLU,\n            norm_layer=None, bias=True, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        bias = to_2tuple(bias)\n\n        self.fc1 = nn.Conv2d(in_features, hidden_features, kernel_size=1, bias=bias[0])\n        self.norm = norm_layer(hidden_features) if norm_layer else nn.Identity()\n        self.act = act_layer()\n        self.drop = nn.Dropout(drop)\n        self.fc2 = nn.Conv2d(hidden_features, out_features, kernel_size=1, bias=bias[1])\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.norm(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        return x\n\n\nclass MlpHead(nn.Module):\n    def __init__(self, dim, num_classes=1000, mlp_ratio=3, act_layer=nn.GELU,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), drop=0., bias=True):\n        super().__init__()\n        hidden_features = int(mlp_ratio * dim)\n        self.fc1 = nn.Linear(dim, hidden_features, bias=bias)\n        self.act = act_layer()\n        self.norm = norm_layer(hidden_features)\n        self.fc2 = nn.Linear(hidden_features, num_classes, bias=bias)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = x.mean((2, 3)) # global average pooling\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.norm(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        return x\n\n\nclass MetaNeXtBlock(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            token_mixer=nn.Identity,\n            norm_layer=nn.BatchNorm2d,\n            mlp_layer=ConvMlp,\n            mlp_ratio=4,\n            act_layer=nn.GELU,\n            ls_init_value=1e-6,\n            drop_path=0.,\n            \n    ):\n        super().__init__()\n        self.token_mixer = token_mixer(dim)\n        self.norm = norm_layer(dim)\n        self.mlp = mlp_layer(dim, int(mlp_ratio * dim), act_layer=act_layer)\n        self.gamma = nn.Parameter(ls_init_value * torch.ones(dim)) if ls_init_value else None\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        shortcut = x\n        x = self.token_mixer(x)\n        x = self.norm(x)\n        x = self.mlp(x)\n        if self.gamma is not None:\n            x = x.mul(self.gamma.reshape(1, -1, 1, 1))\n        x = self.drop_path(x) + shortcut\n        return x\n\n\nclass MetaNeXtStage(nn.Module):\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            ds_stride=2,\n            depth=2,\n            drop_path_rates=None,\n            ls_init_value=1.0,\n            token_mixer=nn.Identity,\n            act_layer=nn.GELU,\n            norm_layer=None,\n            mlp_ratio=4,\n    ):\n        super().__init__()\n        self.grad_checkpointing = False\n        if ds_stride > 1:\n            self.downsample = nn.Sequential(\n                norm_layer(in_chs),\n                nn.Conv2d(in_chs, out_chs, kernel_size=ds_stride, stride=ds_stride),\n            )\n        else:\n            self.downsample = nn.Identity()\n\n        drop_path_rates = drop_path_rates or [0.] * depth\n        stage_blocks = []\n        for i in range(depth):\n            stage_blocks.append(MetaNeXtBlock(\n                dim=out_chs,\n                drop_path=drop_path_rates[i],\n                ls_init_value=ls_init_value,\n                token_mixer=token_mixer,\n                act_layer=act_layer,\n                norm_layer=norm_layer,\n                mlp_ratio=mlp_ratio,\n            ))\n            in_chs = out_chs\n        self.blocks = nn.Sequential(*stage_blocks)\n\n    def forward(self, x):\n        x = self.downsample(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.blocks, x)\n        else:\n            x = self.blocks(x)\n        return x\n\n\nclass MetaNeXt(nn.Module):\n    def __init__(self, in_chans=3, num_classes=1000, depths=(3, 3, 9, 3), dims=(96, 192, 384, 768), token_mixers=InceptionDWConv2d, norm_layer=nn.BatchNorm2d, act_layer=nn.GELU, mlp_ratios=(4, 4, 4, 3), head_fn=MlpHead, drop_rate=0., drop_path_rate=0., ls_init_value=1e-6, **kwargs):\n        super().__init__()\n        # ... (other initialization code as provided)\n        self.stem = nn.Sequential(\n            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n            norm_layer(dims[0])\n        )\n        self.stages = nn.Sequential()\n        dp_rates = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depths)).split(depths)]\n        stages = []\n        prev_chs = dims[0]\n        for i in range(len(depths)):\n            out_chs = dims[i]\n            stages.append(MetaNeXtStage(\n                prev_chs,\n                out_chs,\n                ds_stride=2 if i > 0 else 1,\n                depth=depths[i],\n                drop_path_rates=dp_rates[i],\n                ls_init_value=ls_init_value,\n                act_layer=act_layer,\n                token_mixer=token_mixers,\n                norm_layer=norm_layer,\n                mlp_ratio=mlp_ratios[i],\n            ))\n            prev_chs = out_chs\n        self.stages = nn.Sequential(*stages)\n        self.num_features = prev_chs\n        self.head = head_fn(self.num_features, num_classes, drop=drop_rate)\n        self.apply(self._init_weights)\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        x = self.stages(x)\n        return x\n\n    def forward_head(self, x):\n        x = self.head(x)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n    def _init_weights(self, m):\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            trunc_normal_(m.weight, std=.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\ndef inceptionnext_tiny(**kwargs):\n    model_torch = MetaNeXt(depths=(3, 3, 9, 3), dims=(96, 192, 384, 768), token_mixers=InceptionDWConv2d, **kwargs)\n    My_model = \"/kaggle/input/inceptionnext/pytorch/inceptionnext/1/inceptionnext_tiny.pth\"\n    checkpoint = torch.load(My_model, map_location=torch.device('cuda'))\n    model_torch.load_state_dict(checkpoint.get(\"model\", checkpoint))\n    return model_torch\n\ndef inceptionnext_small(**kwargs):\n    model_torch = MetaNeXt(depths=(3, 3, 27, 3), dims=(96, 192, 384, 768), token_mixers=InceptionDWConv2d, **kwargs)\n    My_model = \"/kaggle/input/inceptionnext/pytorch/inceptionnext/1/inceptionnext_small.pth\"\n    checkpoint = torch.load(My_model, map_location=torch.device('cuda'))\n    model_torch.load_state_dict(checkpoint.get(\"model\", checkpoint))\n    return model_torch\n\ndef inceptionnext_base(**kwargs):\n    model_torch = MetaNeXt(depths=(3, 3, 27, 3), dims=(128, 256, 512, 1024), token_mixers=InceptionDWConv2d, **kwargs)\n    My_model = \"/kaggle/input/inceptionnext/pytorch/inceptionnext/1/inceptionnext_base.pth\"\n    checkpoint = torch.load(My_model, map_location=torch.device('cuda'))\n    model_torch.load_state_dict(checkpoint.get(\"model\", checkpoint))\n    return model_torch\n\ndef inceptionnext_base_384(**kwargs):\n    model_torch = MetaNeXt(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], mlp_ratios=[4, 4, 4, 3], token_mixers=InceptionDWConv2d, **kwargs)\n    My_model = \"/kaggle/input/inceptionnext/pytorch/inceptionnext/1/inceptionnext_base_384.pth\"\n    checkpoint = torch.load(My_model, map_location=torch.device('cuda'))\n    model_torch.load_state_dict(checkpoint.get(\"model\", checkpoint))\n    return model_torch\n\n\n\nif __name__ == '__main__':\n    model_torch = inceptionnext_base()\n    n_parameters = sum(p.numel() for p in model_torch.parameters())\n    print(n_parameters)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T10:28:14.820650Z","iopub.execute_input":"2024-06-20T10:28:14.821163Z","iopub.status.idle":"2024-06-20T10:28:20.597387Z","shell.execute_reply.started":"2024-06-20T10:28:14.821108Z","shell.execute_reply":"2024-06-20T10:28:20.596303Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"86672136\n","output_type":"stream"}]},{"cell_type":"code","source":"checkpoint = torch.load('/kaggle/input/inceptionnext/pytorch/inceptionnext/1/inceptionnext_base.pth', map_location=torch.device('cuda'))\nmodel_torch.load_state_dict(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T10:28:20.599385Z","iopub.execute_input":"2024-06-20T10:28:20.600083Z","iopub.status.idle":"2024-06-20T10:28:20.992683Z","shell.execute_reply.started":"2024-06-20T10:28:20.600043Z","shell.execute_reply":"2024-06-20T10:28:20.991741Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Predict Image**","metadata":{}},{"cell_type":"code","source":"import requests\nfrom PIL import Image\nimport torchvision.models as models\nfrom torchvision import transforms\n\nmodel = inceptionnext_base()\nmodel.load_state_dict(torch.load('/kaggle/input/inceptionnext/pytorch/inceptionnext/1/inceptionnext_base.pth'))\nmodel.eval()\n\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nimage_path = '/kaggle/input/catndog/CuteCat.jpg'\nimage = Image.open(image_path).convert('RGB')\nimage = transform(image)\nimage = image.unsqueeze(0)\n\nurl = 'https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json'\nresponse = requests.get(url)\nclass_idx = response.json()\n\nidx_to_class = {int(key): value[1] for key, value in class_idx.items()}\n\nwith torch.no_grad():\n    output = model(image)\n    _, predicted = torch.max(output, 1)\n    class_id = predicted.item()\n    class_name = idx_to_class[class_id]\n\nprint(f'Predicted class ID: {class_id}, Class name: {class_name}')","metadata":{"execution":{"iopub.status.busy":"2024-06-20T10:28:20.993952Z","iopub.execute_input":"2024-06-20T10:28:20.994673Z","iopub.status.idle":"2024-06-20T10:28:23.908339Z","shell.execute_reply.started":"2024-06-20T10:28:20.994639Z","shell.execute_reply":"2024-06-20T10:28:23.907315Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Predicted class ID: 285, Class name: Egyptian_cat\n","output_type":"stream"}]},{"cell_type":"code","source":"def extract_params(model_torch):\n    params_dict = {}\n    for name, param in model_torch.named_parameters():\n        np_param = param.detach().cpu().numpy()\n        params_dict[name] = np_param\n    return params_dict\n\n# Extract the parameters from the model\nparams = extract_params(model_torch)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T10:28:23.910430Z","iopub.execute_input":"2024-06-20T10:28:23.910733Z","iopub.status.idle":"2024-06-20T10:28:23.922936Z","shell.execute_reply.started":"2024-06-20T10:28:23.910707Z","shell.execute_reply":"2024-06-20T10:28:23.922134Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Conv2D, LayerNormalization, Dropout, Activation, Layer, Concatenate, GlobalAveragePooling1D, GlobalAveragePooling2D\nfrom tensorflow.keras import Model\nimport numpy as np\nimport collections.abc\nfrom itertools import repeat\n\n# Helper function to handle n-tuple\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n            return tuple(x)\n        return tuple(repeat(x, n))\n    return parse\n\nto_2tuple = _ntuple(2)\n\nclass Identity(Layer):\n    def __init__(self):\n        super(Identity, self).__init__()\n\n    def call(self, x):\n        return x\n\nclass DropPath(Layer):\n    def __init__(self, drop_prob=0., scale_by_keep=True):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n        self.keep_prob = 1 - drop_prob\n        self.scale_by_keep = scale_by_keep\n\n    def call(self, x, training=None):\n        if self.drop_prob == 0. or not training:\n            return x\n        keep_prob = 1 - self.drop_prob\n        random_tensor = tf.random.uniform(shape=tf.shape(x))\n        random_tensor = tf.floor(random_tensor + keep_prob)\n        if self.scale_by_keep:\n            random_tensor = random_tensor / keep_prob\n        return x * random_tensor\n\nclass InceptionDWConv2d(Layer):\n    def __init__(self, in_channels, square_kernel_size=3, band_kernel_size=11, branch_ratio=0.125):\n        super(InceptionDWConv2d, self).__init__()\n        gc = int(in_channels * branch_ratio)\n        self.dwconv_hw = Conv2D(gc, square_kernel_size, padding='same', groups=gc)\n        self.dwconv_w = Conv2D(gc, (1, band_kernel_size), padding='same', groups=gc)\n        self.dwconv_h = Conv2D(gc, (band_kernel_size, 1), padding='same', groups=gc)\n        self.split_indexes = (in_channels - 3 * gc, gc, gc, gc)\n\n    def call(self, x):\n        x_id, x_hw, x_w, x_h = tf.split(x, self.split_indexes, axis=-1)\n        return Concatenate(axis=-1)([x_id, self.dwconv_hw(x_hw), self.dwconv_w(x_w), self.dwconv_h(x_h)])\n\nclass ConvMlp(Layer):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer='relu', norm_layer=None, bias=True, drop=0.):\n        super(ConvMlp, self).__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n\n        self.fc1 = Conv2D(hidden_features, 1, use_bias=bias)\n        self.norm = norm_layer() if norm_layer else Identity()\n        self.act = Activation(act_layer)\n        self.drop = Dropout(drop)\n        self.fc2 = Conv2D(out_features, 1, use_bias=bias)\n\n    def call(self, x):\n        x = self.fc1(x)\n        x = self.norm(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\nclass MlpHead(Layer):\n    def __init__(self, dim, num_classes=1000, mlp_ratio=3, act_layer='gelu', norm_layer=LayerNormalization, drop=0., bias=True):\n        super(MlpHead, self).__init__()\n        hidden_features = int(mlp_ratio * dim)\n        self.fc1 = Dense(hidden_features, use_bias=bias)\n        self.act = Activation(act_layer)\n        self.norm = norm_layer(epsilon=1e-6)\n        self.fc2 = Dense(num_classes, use_bias=bias)\n        self.drop = Dropout(drop)\n\n    def call(self, x):\n        x = tf.reduce_mean(x, axis=[1, 2])  # global average pooling\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.norm(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        return x\n\nclass MetaNeXtBlock(Layer):\n    def __init__(self, dim, token_mixer_class, norm_layer=LayerNormalization, mlp_layer=ConvMlp, mlp_ratio=4, act_layer='relu', ls_init_value=1e-6, drop_path=0.):\n        super(MetaNeXtBlock, self).__init__()\n        self.token_mixer = token_mixer_class(dim)\n        self.norm = norm_layer()\n        self.mlp = mlp_layer(dim, int(mlp_ratio * dim), act_layer=act_layer)\n        self.gamma = self.add_weight(shape=(dim,), initializer=tf.keras.initializers.Constant(ls_init_value), trainable=True) if ls_init_value else None\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else Identity()\n\n    def call(self, x):\n        shortcut = x\n        x = self.token_mixer(x)\n        x = self.norm(x)\n        x = self.mlp(x)\n        if self.gamma is not None:\n            x = x * self.gamma[None, None, None, :]\n        x = self.drop_path(x) + shortcut\n        return x\n\nclass MetaNeXtStage(Layer):\n    def __init__(self, in_chs, out_chs, ds_stride=2, depth=2, drop_path_rates=None, ls_init_value=1.0, token_mixer_class=InceptionDWConv2d, act_layer='relu', norm_layer=LayerNormalization, mlp_ratio=4):\n        super(MetaNeXtStage, self).__init__()\n        if ds_stride > 1:\n            self.downsample = tf.keras.Sequential([\n                norm_layer(),\n                Conv2D(out_chs, kernel_size=ds_stride, strides=ds_stride)\n            ])\n        else:\n            self.downsample = Identity()\n\n        drop_path_rates = drop_path_rates or [0.] * depth\n        self.blocks = [MetaNeXtBlock(dim=out_chs, drop_path=drop_path_rates[i], ls_init_value=ls_init_value, token_mixer_class=token_mixer_class, act_layer=act_layer, norm_layer=norm_layer, mlp_ratio=mlp_ratio) for i in range(depth)]\n\n    def call(self, x):\n        x = self.downsample(x)\n        for block in self.blocks:\n            x = block(x)\n        return x\n\nclass MetaNeXt(Model):\n    def __init__(self, in_chans=3, num_classes=1000, depths=(3, 3, 9, 3), dims=(96, 192, 384, 768), token_mixers=InceptionDWConv2d, norm_layer=LayerNormalization, act_layer='relu', mlp_ratios=(4, 4, 4, 3), head_fn=MlpHead, drop_rate=0., drop_path_rate=0., ls_init_value=1e-6):\n        super(MetaNeXt, self).__init__()\n\n        num_stage = len(depths)\n        if not isinstance(token_mixers, (list, tuple)):\n            token_mixers = [token_mixers] * num_stage\n        if not isinstance(mlp_ratios, (list, tuple)):\n            mlp_ratios = [mlp_ratios] * num_stage\n\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        self.stem = tf.keras.Sequential([\n            Conv2D(dims[0], kernel_size=4, strides=4),\n            norm_layer()\n        ])\n\n        dp_rates = [x.numpy().tolist() for x in tf.split(tf.linspace(0., drop_path_rate, sum(depths)), num_or_size_splits=depths)]\n        self.stages = []\n        prev_chs = dims[0]\n        for i in range(num_stage):\n            out_chs = dims[i]\n            self.stages.append(MetaNeXtStage(prev_chs, out_chs, ds_stride=2 if i > 0 else 1, depth=depths[i], drop_path_rates=dp_rates[i], ls_init_value=ls_init_value, act_layer=act_layer, token_mixer_class=token_mixers[i], norm_layer=norm_layer, mlp_ratio=mlp_ratios[i]))\n            prev_chs = out_chs\n        self.num_features = prev_chs\n        self.head = head_fn(self.num_features, num_classes, drop=drop_rate)\n\n    def call(self, x):\n        x = self.stem(x)\n        for stage in self.stages:\n            x = stage(x)\n        x = self.head(x)\n        return x\n    \n    \n\ndef inceptionnext_tiny(**kwargs):\n    model = MetaNeXt(depths=(3, 3, 9, 3), dims=(96, 192, 384, 768),\n                     token_mixers=InceptionDWConv2d,\n                      **kwargs\n    )\n    return model\n\ndef inceptionnext_small(**kwargs):\n    model = MetaNeXt(depths=(3, 3, 27, 3), dims=(96, 192, 384, 768),\n                     token_mixers=InceptionDWConv2d,\n                      **kwargs\n    )\n    return model\n\ndef inceptionnext_base(**kwargs):\n    model = MetaNeXt(depths=(3, 3, 27, 3), dims=(128, 256, 512, 1024),\n                     token_mixers=InceptionDWConv2d,\n                      **kwargs\n    )\n    return model\n\ndef inceptionnext_base_384(**kwargs):\n    model = MetaNeXt(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024],\n                     mlp_ratios=[4, 4, 4, 3],\n                     token_mixers=InceptionDWConv2d,\n                      **kwargs\n    )\n    return model\n\nmodel_tf = inceptionnext_base()\ninput_tensor = tf.random.normal([1, 224, 224, 3])\noutput = model_tf(input_tensor)\nn_parameters = np.sum([np.prod(v.shape) for v in model_tf.trainable_weights])\nprint(n_parameters)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T10:28:23.924565Z","iopub.execute_input":"2024-06-20T10:28:23.925188Z","iopub.status.idle":"2024-06-20T10:28:51.763667Z","shell.execute_reply.started":"2024-06-20T10:28:23.925154Z","shell.execute_reply":"2024-06-20T10:28:51.762379Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-06-20 10:28:24.270011: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-20 10:28:24.270075: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-20 10:28:24.271522: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1718879323.297156    3989 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"86672136\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Predict Image**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nimport numpy as np\nimport json\nimport requests\n\njson_url = \"https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\"\n\n\nresponse = requests.get(json_url)\nclass_index = response.json()\n\ndef get_class_label(index):\n    return class_index[str(index)][1]\n\ndef load_and_preprocess_image(img_path, target_size=(384, 384)):\n    img = tf.keras.utils.load_img(img_path, target_size=target_size)\n    img_array = tf.keras.utils.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    img_array = img_array / 255.0\n    return img_array\n\ndef predict_image_class(img_path):\n    preprocessed_image = load_and_preprocess_image(img_path)\n    predictions = model_tf(preprocessed_image)\n    predicted_class = np.argmax(predictions, axis=-1)[0]\n    predicted_label = get_class_label(predicted_class)\n    return predicted_class, predicted_label\n\nimg_path = '/kaggle/input/catndog/CuteCat.jpg'\npredicted_class, predicted_label = predict_image_class(img_path)\nprint(f\"Predicted class index: {predicted_class}, label: {predicted_label}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-20T10:28:51.765273Z","iopub.execute_input":"2024-06-20T10:28:51.766019Z","iopub.status.idle":"2024-06-20T10:29:00.478970Z","shell.execute_reply.started":"2024-06-20T10:28:51.765980Z","shell.execute_reply":"2024-06-20T10:29:00.477883Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Predicted class index: 45, label: Gila_monster\n","output_type":"stream"}]},{"cell_type":"code","source":"# for i, layer in enumerate(model_tf.layers):\n#     print(i, layer.name, [w.shape for w in layer.get_weights()])","metadata":{"execution":{"iopub.status.busy":"2024-06-20T10:29:00.480810Z","iopub.execute_input":"2024-06-20T10:29:00.481249Z","iopub.status.idle":"2024-06-20T10:29:00.485164Z","shell.execute_reply.started":"2024-06-20T10:29:00.481209Z","shell.execute_reply":"2024-06-20T10:29:00.484207Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef map_weights_to_tf(tf_model, params):\n    # Function to set weights for dense layers\n    def set_dense_weights(tf_layer, pt_weight_key, pt_bias_key):\n        dense_weight = params[pt_weight_key]\n        dense_weight_reshaped = np.transpose(dense_weight, (1, 0))  # Convert from PyTorch to TensorFlow format\n        dense_bias = params[pt_bias_key]\n        tf_layer.set_weights([dense_weight_reshaped, dense_bias])\n    \n    # Function to set weights for conv layers\n    def set_conv_weights(tf_layer, pt_weight_key, pt_bias_key=None):\n        conv_weight = params[pt_weight_key]\n        conv_weight_reshaped = np.transpose(conv_weight, (2, 3, 1, 0))  # Convert from PyTorch to TensorFlow format\n        if pt_bias_key and pt_bias_key in params:\n            conv_bias = params[pt_bias_key]\n            tf_layer.set_weights([conv_weight_reshaped, conv_bias])\n        else:\n            tf_layer.set_weights([conv_weight_reshaped])\n\n    # Function to set weights for batch norm layers\n    def set_bn_weights(tf_layer, pt_weight_key, pt_bias_key, pt_running_mean_key=None, pt_running_var_key=None):\n        bn_weight = params[pt_weight_key]\n        bn_bias = params[pt_bias_key]\n        if pt_running_mean_key and pt_running_var_key and pt_running_mean_key in params and pt_running_var_key in params:\n            bn_running_mean = params[pt_running_mean_key]\n            bn_running_var = params[pt_running_var_key]\n            tf_layer.set_weights([bn_weight, bn_bias, bn_running_mean, bn_running_var])\n        else:\n            tf_layer.set_weights([bn_weight, bn_bias])\n    \n    # Function to set weights for layer norm layers\n    def set_ln_weights(tf_layer, pt_weight_key, pt_bias_key):\n        ln_weight = params[pt_weight_key]\n        ln_bias = params[pt_bias_key]\n        tf_layer.set_weights([ln_weight, ln_bias])\n    \n    # Set weights for the stem layer\n    set_conv_weights(tf_model.stem.layers[0], 'stem.0.weight', 'stem.0.bias')\n    set_bn_weights(tf_model.stem.layers[1], 'stem.1.weight', 'stem.1.bias', 'stem.1.running_mean', 'stem.1.running_var')\n\n    # Set weights for each stage\n    for i, stage in enumerate(tf_model.stages):\n        for j, block in enumerate(stage.blocks):\n            block_prefix = f'stages.{i}.blocks.{j}'\n            set_conv_weights(block.token_mixer.dwconv_hw, f'{block_prefix}.token_mixer.dwconv_hw.weight', f'{block_prefix}.token_mixer.dwconv_hw.bias')\n            set_conv_weights(block.token_mixer.dwconv_w, f'{block_prefix}.token_mixer.dwconv_w.weight', f'{block_prefix}.token_mixer.dwconv_w.bias')\n            set_conv_weights(block.token_mixer.dwconv_h, f'{block_prefix}.token_mixer.dwconv_h.weight', f'{block_prefix}.token_mixer.dwconv_h.bias')\n            set_ln_weights(block.norm, f'{block_prefix}.norm.weight', f'{block_prefix}.norm.bias')\n            set_conv_weights(block.mlp.fc1, f'{block_prefix}.mlp.fc1.weight', f'{block_prefix}.mlp.fc1.bias')\n            set_conv_weights(block.mlp.fc2, f'{block_prefix}.mlp.fc2.weight', f'{block_prefix}.mlp.fc2.bias')\n        \n        if hasattr(stage, 'downsample') and isinstance(stage.downsample, tf.keras.Sequential):\n            downsample_prefix = f'stages.{i}.downsample'\n            set_conv_weights(stage.downsample.layers[1], f'{downsample_prefix}.1.weight', f'{downsample_prefix}.1.bias')\n            set_bn_weights(stage.downsample.layers[0], f'{downsample_prefix}.0.weight', f'{downsample_prefix}.0.bias', f'{downsample_prefix}.0.running_mean', f'{downsample_prefix}.0.running_var')\n    \n    # Set weights for the head layer\n    set_dense_weights(tf_model.head.fc1, 'head.fc1.weight', 'head.fc1.bias')\n    set_ln_weights(tf_model.head.norm, 'head.norm.weight', 'head.norm.bias')\n    set_dense_weights(tf_model.head.fc2, 'head.fc2.weight', 'head.fc2.bias')","metadata":{"execution":{"iopub.status.busy":"2024-06-20T10:29:00.486910Z","iopub.execute_input":"2024-06-20T10:29:00.487405Z","iopub.status.idle":"2024-06-20T10:29:00.506004Z","shell.execute_reply.started":"2024-06-20T10:29:00.487354Z","shell.execute_reply":"2024-06-20T10:29:00.504929Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"map_weights_to_tf(model_tf, params)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T10:29:00.507197Z","iopub.execute_input":"2024-06-20T10:29:00.507543Z","iopub.status.idle":"2024-06-20T10:29:01.353428Z","shell.execute_reply.started":"2024-06-20T10:29:00.507515Z","shell.execute_reply":"2024-06-20T10:29:01.352603Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model_tf.save_weights('model.weights.h5')","metadata":{"execution":{"iopub.status.busy":"2024-06-20T10:29:01.356334Z","iopub.execute_input":"2024-06-20T10:29:01.356664Z","iopub.status.idle":"2024-06-20T10:29:02.697479Z","shell.execute_reply.started":"2024-06-20T10:29:01.356638Z","shell.execute_reply":"2024-06-20T10:29:02.696408Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model_tf.load_weights('/kaggle/working/model.weights.h5')","metadata":{"execution":{"iopub.status.busy":"2024-06-20T10:29:02.699028Z","iopub.execute_input":"2024-06-20T10:29:02.699462Z","iopub.status.idle":"2024-06-20T10:29:03.621441Z","shell.execute_reply.started":"2024-06-20T10:29:02.699425Z","shell.execute_reply":"2024-06-20T10:29:03.620562Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"n_parameters = np.sum([np.prod(v.shape) for v in model_tf.trainable_weights])\nprint(n_parameters)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T10:29:03.622994Z","iopub.execute_input":"2024-06-20T10:29:03.623419Z","iopub.status.idle":"2024-06-20T10:29:03.640400Z","shell.execute_reply.started":"2024-06-20T10:29:03.623383Z","shell.execute_reply":"2024-06-20T10:29:03.639344Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"86672136\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\ndef preprocess_image(img_path, target_size=(224, 224)):\n    img = image.load_img(img_path, target_size=target_size)\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    img_array = tf.keras.applications.imagenet_utils.preprocess_input(img_array)\n    return img_array\n\nclass_index_path = 'imagenet_class_index.json'\nif not os.path.exists(class_index_path):\n    url = 'https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json'\n    response = requests.get(url)\n    with open(class_index_path, 'wb') as f:\n        f.write(response.content)\n\n# Load the ImageNet class index\nwith open(class_index_path) as f:\n    class_index = json.load(f)\n\nimg_path = '/kaggle/input/catndog/CuteCat.jpg'\n\nimg_array = preprocess_image(img_path)\n\npredictions = model_tf.predict(img_array)\npredicted_class_index = np.argmax(predictions, axis=-1)[0]\n\npredicted_class_name = class_index[str(predicted_class_index)][1]\n\nprint(f'Predicted class index: {predicted_class_index}, class name: {predicted_class_name}')","metadata":{"execution":{"iopub.status.busy":"2024-06-20T10:29:03.641935Z","iopub.execute_input":"2024-06-20T10:29:03.642213Z","iopub.status.idle":"2024-06-20T10:29:19.409684Z","shell.execute_reply.started":"2024-06-20T10:29:03.642188Z","shell.execute_reply":"2024-06-20T10:29:19.408685Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 16s/step\nPredicted class index: 163, class name: bloodhound\n","output_type":"stream"}]}]}